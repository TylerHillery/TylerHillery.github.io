{
  "hash": "5f6b7967f1aaffc39295c333bda3b832",
  "result": {
    "markdown": "---\ntitle: \"Building My Own Personal Data Cabin - Part 1: The Blueprint \"\ndescription: My data cabin is a personal data lakehouse for me to go explore new technologies and ideas.  \nauthor: \"Tyler Hillery\"\ndate: \"2023-05-25\"\ncategories: [Data Lakehouse, Iceberg]\nimage: \"../../assets/images/DataLakeHouse.png\"\nfilters:\n  - social-share\nshare:\n  permalink: \"https://tylerhillery.com/blog/tylers-data-lakehouse/\"\n  description:\n  twitter: true\n  facebook: false\n  reddit: true\n  stumble: false\n  tumblr: false\n  linkedin: true\n  email: true\n  mastodon: true\n---\n\n## **Overview**\n\nGrowing up in Wisconsin a common nomenclature is to say you are \"going up north\" for the weekend. This may seem odd to most given that Wisconsin is already bordering Canada so how much further north would you want to go? Now this depends where you grew up but me personally I considered \"up north\" a city called Tomahawk or anything further north of it. \n\nA common thing to do up north is to go to a cabin on a lake. It's actually a dream of most to own their own cabin on a lake somewhere up north. I too have always wanted to have my own cabin but not a *real* cabin, a data cabin. A place I can go to in my spare time to toy around with different data technologies and explore ideas I have.\n\nYou may be wondering what the difference between a data lakehouse and \"data cabin\" is and there isn't any difference. I am simply using the term interchangeably but I found the term quite fitting given the fact that I view real lakehouses as place you would live in everyday and have a little bit higher expectations in the build quality of the house while a cabin has more character, a little rough around the edges, a place you visit ever so often to get away from the typical mundane day to day activities. \n\nYou can translate these comparisons to the data world, where a data lakehouse is a real production system that has been battle tested and can easily scale to support BIG data. My cabin on the other hand.. it's a place for me to get away from my daily job responsibilities to explore technologies and ideas that I find interesting. Before we get into the design of my cabin lets discuss the my definition of the data lakehouse architecture. \n\n### **The Data Lakehouse**\nThe term *Data Lakehouse* came from combining the words *Data Lake* and *Data Warehouse*. Lets first describe how I view these two terms.\n\n#### **The Data Lake**\nThe Data Lake is the term used describe storing all your data into object storage like S3. This can be anything from structured data (parquet, CSVs etc.) to semi-structured data (JSON, XML) to unstructured data (images, audio etc,). To me that is it, that's a data lake. \n\n#### **The Data Warehouse**\nThe Data Warehouse is a database that is suppose to act as the coveted *single source of truth* of all data within a company. While you can use trusty old Postgres as Craig Kerstiens demonstrates:\n\n\n{{< tweet craigkerstiens 1651336279806853120 >}}\n\n\n\nThe more common players you are going to see fulfill the data warehouse role is Snowflake, BigQuery & Redshift. That's because the workloads or types of query you run on data warehouses tend to be different in nature than the queries you run on your application database. The term for this type of workload is commonly referred to as OLAP (Online Analytical Processing) which typically involves running aggregate queries across the entire dataset where as OLTP (Online Transactional Processing) is usually more concerned with single point queries where you are concerned with reading or updating one single entity. This is drastic simplification but it gets the point across.\n\nThe reason the above mention names tend to be suited for OLAP is largely because they are distributed systems designed so that they can horizontally scale resources. The storage and compute are separated as well so they scale independently of each other so you don't necessarily need to increase storage if you just want more compute resources. The distributed nature of the computer resources allows for massively parallel processing (MPP) which allows the database to split the work across several different nodes within the cluster. \n\n\n::: {layout-ncol=1}\n![Snowflake Arch](../../assets/images/snowflake-arch.png)\n:::\n\n\n```{markdown}\n#| label: fig-cap-margin\n#| fig-cap: \"MPG vs horsepower, colored by transmission.\"\n#| cap-location: top\n\n![](../../assets/images/snowflake-arch.png)\n*https://docs.snowflake.com/en/user-guide/intro-key-concepts*\n```\n\n\nThe other design choice that makes these databases suited for OLAP workloads is the columnar data store. This allows for compression because typically one column is going to be of the same data type and also usually allows the user to scan less data because a lot of times your query is only going to select a few columns. \n\n### **Brain Dump of all the technologies I was to try**\n- Iceberg\n    - Catalog\n        - Undecided between \n            - Rest\n            - Hive Metastore (AWS Glue Catalog)\n            - Nessie ✅\n- Workflow Orchestrator\n    - I want to use both Dagster and Airflow. \n        - How airflow run inside Dagster like they marketed before\n\n- Special dbt project\n    - Uses duckdb for dev \n        - Complete with CI/CD\n            - Data Diff\n            - SQL Fluff\n            - SQL Glot to translate DuckDB to Spark\n    - Uses something else for prod (unsure what yet, maybe spark?)\n        - I am thinking for Prod it's got to be Spark as this has the best support for dbt and Iceberg\n            - AWS Glue ✅\n            - AWS Serverless EMR\n- Data Ingestion\n    - Use Buz for run time data validation\n    - Real-Time\n        - Message Broker\n            - Kafka\n            - Redpanda\n        - Real Time Processing Engine\n            - Bytewax\n            - Flink\n        - Real time database \n            - Materialize\n        - I want to try CDC with debezium\n            - I will need a OLTP database so I am think I go with Crunchy Bridge to setup a postgres instance\n    - I want to try event collector like Snowplow\n    - For one of the Real Time pipelines I was to try to recreate Real Time version of the Coinbase Pro \n    - Airbyte with a custom connector \n\n- Business Intelligence\n    - Metabase\n    - Evidence  \n    - Rill \n\n- Metrics Store\n    - Cube\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}