---
title: "Designing Data-Intensive Applications"
description: My notes on [Designing Data-Intensive Applications](https://a.co/d/7r8FaLD) by Martin Kleppmann.
author: "Tyler Hillery"
date: "2023-10-23"
image: "./ddia.jpg"
filters:
  - social-share
share:
  permalink: "https://tylerhillery.com/notes/ddia/"
  description:
  twitter: true
  facebook: false
  reddit: true
  stumble: false
  tumblr: false
  linkedin: true
  email: true
  mastodon: true
categories: [Technical Books]
---
___

I am going to try something new while reading this book. After every reading session I will journal any thoughts and ideas I have, unfiltered and unedited. This book comes highly regarded and I want to be able to capture my experience during my first read through. 

::: {.callout-caution appearance="default" collapse=false title="TODO"}
These notes are a work in progress and may have several typos!
:::

# Reading Session 01
- **date**: Tue 10/10/2023
- **time**: 7:05PM-7:15PM (10 min)
- **chapter**: Preface  
- **pages**: 5-18 (15 pages)

The preface sets the stage for the book describing the scope, who should read the book, and the outline of the book. Based on the preface alone I have feeling I am going to really enjoy it. The preface emphasizes the focus of the book to be on the principals of designing data intensive applications. We first learn also what does a data intensive application is:

> We call an application ***data-intensive*** if the data is its primary challenge—the quantity of data, the complexity of data, or the speed at which it is changing—as opposed to ***compute-intensive***, where CPU cycles are the bottleneck.

I personally identified with “*the people who have natural curiosity for the way things work*” as my reasoning for reading this book. My reasoning goes along with this point as well:

> You will, however, develop a good intuition for what your systems are doing under the hood so that you can reason about their behavior, make good design decisions, and track down any problems that may arise.

For me it’s nice to be able to have the high level mental model of how some of these systems works so I can mentally connect the dots even though I may not be working on these underlying systems directly. Although one day I would like to 😉

I am looking forward to reading the rest of the book!

___

# Reading Session 02
- **date**: Wed 10/11/2023
- **time**: 4:00AM-4:45AM (45 min) 
- **chapter**: Ch 01 - Reliability 
- **pages**: 18-31 (13 pages)

## Notes
I like the concept of *data systems* even today we are still seeing the lines get blurred further and further between messaging systems and databases. For example Redpanda recently announced that they plan on adding iceberg support for the tiered storage. This would me any query engine that can read the Iceberg table format should be able to query the data. Does this make Redpanda a database? I think so. 

Many of us may be data system designers without even realizing it. Figure 1.1. does a great job showing how one application could be interacting with an in memory cache database, a primary db, a full-text index and a message queue all at once. all glued together through the application code with the implementation details hidden from clients. I think this book is more wildly applicable than people realize. 

Three key terms:

> ***Reliability***: The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error).

> ***Reliability***: As the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth

> ***Maintainability***: Over time, many different people will work on the system (engineering and operations, both maintaining current behavior and adapting the system to new use cases), and they should all be able to work on it productively.

> The things that can go wrong are called *faults*, and systems that anticipate faults can cope with them are called **fault-tolerant** or **resilient**.

::: {.callout-important appearance="default" collapse=false title="TODO"}
Look into the **Netflix Chaos Monkey** (something about purposely introducing faults to test the system)
:::

::: {.callout-important appearance="default" collapse=false title="TODO"}
Look up **RAID** configuration, I remember learning about this in my computer org. and arch. class
:::

Interesting to hear how handling reliability is now less focuses on making the hardware more redundant and instead having the software handle it. This was largely due to the cause of cloud platforms like AWS which have a focus more on flexibility and elastic over single-machine reliability.

When the author was giving example of software faults and how they can lie dormant for a long time until they are triggered by an unusual set of circumstances. It reminded me of a bug I encountered yesterday for our dim_calendar table which had a column called is_holiday. My assumption when using this table was than when is_holiday was `true` that would also mean the stock markets would be close. Well in the case of 10/09/2023, Columbus day is a federal holiday but the markets are still open. Lucky enough I had some dbt tests that notified me there was something wrong with some downstream dependencies of this table. I was able to quickly search through the DAG and noticed that the is_holiday column was based on a python UDF that used `from [pandas.tseries.holiday](http://pandas.tseries.holiday) import USFederalHolidayCalendar` . I was able to modify the function to only use holidays observed by the stock market and open PR to fix this.

## Reading session summary
My first thought is this book is going to take forever if I continue taking these detailed notes and journaling about thoughts as I read. I expect other sessions to be less detailed but I am going to try to stick with it. It was nice to learn more about what reliability means and how to handle it. I believe this is an area of mine and that I need to work on. Most of the work I do is more reactive (testing) to faults that can cause me to go into a panic trying to debug what is wrong. It would be better if I could be more proactive. It reminds me of a pattern used in data engineering called Write-Audit-Publish (WAP) wow, this acronym is forever ruined by Cardi B. In essence WAP involves:

1. Write the data to a staging area (a non-production env so the data is isolated)
2. Audit the data to validate the data and solve any quality issues (NULL values, duplicates etc.)
3. publish data to production
        
Table formats like Iceberg even have special functionality to help implement this pattern like the older version `WAP.id` or the newer method with branching `WAP.branch` . More about this can be found here [Streamlining Data Quality in Apache Iceberg with write-audit-publish & branching](https://www.dremio.com/blog/streamlining-data-quality-in-apache-iceberg-with-write-audit-publish-branching/#:~:text=Write%2DAudit%2DPublish%20(WAP)%20is%20a%20data%20quality,data%20to%20the%20production%20tables)

This pattern reminds me of version control. Where step one is like creating your own local branch, step two is the CI/CD process that runs testing your changes. Once passed step three happens where changes are deployed to production.

___

# Reading Session 03
- **date**: Thu 10/12/2023
- **time**: 6:45PM-7:00PM (15 min)
- **chapter**: Ch 01 - Scalability
- **pages**: 31-39 (8 pages)

## Notes
I like the author phrase how it’s not just good enough to say this systems scales, we have to ask “*what are our options for coping with growth*”

Load can be described with load parameters
- requests per second to a web server
- ratio of reads to writes to a db
- number if simultaneously active users in a chat room
- hit rate on a cache

Interesting to hear that Twitter’s scaling challenge is due to fan-out where each user follows many people and each user is followed by many people.

Love the inside joke on just setting up my twttr.

> ***Throughput***: The number of records we can process per second, or the total time it takes to run a job on a dataset of a certain size.

> ***Response time***: What the client sees: besides the actual time to process the request (the service time), it includes network delays and queueing delays. 

> ***Latency***: Duration that a request is waiting to be handled

Response time can vary greatly to it’s best to measure as a distribution of values.

 Another good tidbit, mean is not good measure for “*typical*” response time, because it doesn’t tell you how many users actually experience delay. It’s better to use percentiles and use the *median* aka the *50th percentile*.

## Reading Session Summary
I’ll be honest I didn’t want to read today but I currently have my longest reading streak in the kindle app (6 days) and I didn’t want to break it. My original plan was to read just one page but it’s surprising once you start something you don’t want to stop. Author poses some many great questions to ask when comes to scalability of systems and really starts to question key terms I have heard of better could never define or put into words if you asked me, like what is *load* or *load parameters*.

___

# Reading Session 04
- **date**: Fri 10/13/2023
- **time**: 4:05AM-4:20AM (15 min)  
- **chapter**: Ch 01 - Scalability
- **pages**: 39-46 (7 pages)

## Reading Session Summary
Couple things that I came to mind as I finished up the scalability portion. I like how the author started with first describing load and metrics for measure for performance before jumping straight into “*how to maintain good performance even when load parameters increase by some amount?*” I believe this is a thoughtful exercise to do before trying to optimize for scalability because it forces you to think about what it is you are actually trying to optimizer for. This deep thinking will help avoid any pitfalls of optimizing for the wrong thing, the author even calls out “*An architecture that scales well for particular application is built around assumptions of which operations will be common and which will be are — the load parameters*”

I am familiar will some of the common techniques mentioned in this chapter with ways dealing scalability such as vertical and horizontal scaling and shared-nothing architecture. The author makes the correct call that distributed data systems have become the default choice. In fact I think as of today people choose distributed systems far tool quickly as the default the choice when tackling data problems. I recently came across a post on r/dataengineering where an engineer was helping out a local floral shop and they set them up with BigQuery, Mage, dbt etc. It wasn’t clear to me the size of data they were dealing, and with but the technology choices they recommended just seemed absurd. Just thinking about my local floral and setting them up with this tech, who the heck is going to maintain it? Also the problem didn’t really call for all these tools, they simply needed a more automated way of cross referencing data from SQL server and manually downloaded sales information. This sounds like something a spreadsheet could do. The pendulum has shifted too far as making these distributed systems the default choice. In recent times with the rise of tools like DuckDB, I believe people are going to second guess themselves before instinctively setting up a spark cluster to do some simple ELT/ELT work. 

___

# Reading Session 05
- **date**: Sat 10/14/2023
- **time**: 4:05AM-4:20AM (15 min)
- **chapter**: Ch 01 - Maintainability & Ch 02 - Data Models
- **pages**: 39-46 (7 pages)

## Notes

3 design principals of **Maintainability**

> ***Simplicity***: Make it easy for new engineers to understand the system, by removing as much complexity as possible from the system. 

- This can be tough finding the right level of abstraction, author actually goes into detail about abstraction and relates it to high-level programming languages with how they abstract away having to write machine code. There is some foreshadowing by the author implying we will learn some techniques into how to find the right layer of abstraction.

> ***Operability***: Make it easy for operations team to keep the system running smoothly

> ***Simplicity***:  Make it easy for engineers to make changes to the system in the future, adapting for unanticipated use cases as requirements change.   

Don’t create *big balls of mud*. 

## Reading Session Summary
Finishing up chapter 1 I learned what it means to create maintainable software. I have personally been there, working on some code and thinking to myself “*who wrote this awful code?*” Only to check the commit history to see it was me… Refactoring code honestly feels like a majority of the work I have done in my career, it always feels like no matter what company I worked for or what role I was in I was taking old code and making it better or migrating it to another platform. These personal experiences just reiterate the point of how crucial it is to create maintainable systems and I liked how the author broke down maintainable into 3 design principals. Overall, Chapter 1 did a great job on laying the foundation and principles that go into thinking about data-intensive applications

When I read the title of Chapter 2. Data Models and Query Languages I got excited. This is a particular interest of mine. The author makes a bold claim about data models

> Data Models are perhaps the most important part of developing software because they have such a profound effect: not only on how the software is written, but also on how we *think about the problem* that we solving

I have long had this belief that all software can be distilled down to taking data in, manipulating it and producing data as a result. I am something of a self proclaimed **data extremist**.  This is why I am so fascinated by databases and data systems as a whole. Now I come from a analytics background and what a lot people think I am describing here is a data pipeline but I also think this pattern is just as applicable to all domains in tech; front-end engineering, back-end engineering, data engineering, systems engineering. Everyone at the end of the no matter what they are building are taking data in, and doing something with the data and producing some sort of output which is also data. Front-end engineer gets JSON data from an API, transforms it into HTML (which is textual data) and renders it on the client. It’s the data models and the ways we represent & interact with the data that separate these engineers into these domains.

I like the way the author phrases how applications are really built by layering one data model on top of another which really get be thought of as an abstraction layer IMO. I think it’s important at whatever layer you are operating at to try learning the skills and tools commonly used one layer up and below as it will make you that much more knowledgeable about operating at the layer you normally work in. This is part of the reason that of why I am learning about lower level systems and how databases actually work. As an analytics engineering my main reasonability is to best figure out how to best model our data to enable down stream uses cases like machine learning & reporting, commonly implemented by using SQL. Understanding how databases work will give me better insight into writing more optimize and performant queries even if I never actually have to build a database.

It was interesting to hear that databases back in the day made application developers think about the internal representation of the data in the database. Could imagine actually having to worry about how the data is being physically stored on disked opposed to just writing `create table` and `insert values`? Wow what a time to be alive.

Also I never knew of the terms: ***business data processing***, ***network model*** and ***hierarchical model***. Just goes to show how the ***relational model*** has stood the test of time & appears to have made some of these other models obsolete.

___

# Reading Session 06
- **date**: Sun 10/15/2023
- **time**: 3:20PM-3:55PM (35 min)
- **chapter**: Ch 02 Data Models & Query Languages
- **pages**: 61-121 (60 pages)

## Notes

One comparison that I really liked was:

> *Schema-on-read* is similar to dynamic (runtime) type checking in programming languages, whereas *schema-on-write* is similar to static (compile-time) type checking.

::: {.callout-important appearance="default" collapse=false title="TODO"}
Look into the **Google's Spanner database** (how does it offer same locality properties in a relational model?)
:::

> *imperative language* tells the computer to perform certain operations in order

> *declarative language* describe the result you want 

Declarative languages are better for parallel execution and hides implementation details.  

## Reading Session Summary
I am most familiar with the relational model so hearing about other data models and NoSQL systems was interesting to learn about. I still believe the relational model is the best and continues to improve as support for things like JSON get better. The author also mentions this point in the book how relation model systems and non-relation models systems are converging in terms of capabilities. 

___

# Reading Session 07
- **date**: Mon 10/16/2023
- **time**: 12:45PM-1:45PM (60 min)
- **chapter**: Ch 03 Storage and Retrieval
- **pages**: 121-169 (48 pages)

## Notes

One of the main reasons I am reading this book:

> You're probably not going to implement your own storage engine from scratch, but you *do* need to select a storage engine that is appropriate for your application, from the many that are available. In order to tune a storage engine well on your kind of workload, you need to have a rough idea of what the storage engine is doing under the hood.

I never realized how powerful an append only log is.

Not many people realize this about columnar stores

> Note that it wouldn't make sense to sort each column independently, because then we would no longer know which items in the columns belong to the same row. We can only reconstruct a row because we know that the *k*th item in one column belongs to the same row as the *k*th item in another column.
>
>Rather, the data needs to be sorted as an entire row at a time, even though it is stored by column. 

Redshift allow to define the sort order by created a *sort key*. This can help speed up queries by pruning data that is not need to be read based on the min and max values of certain zone maps. Very similar to how an index works. You commonly want to sort on a field used in `where` clauses. A good example would be a `date` column. 

Interesting to learn about the idea of having several different sort orders and letting the database figure out the best one. I wonder if that's what these "*Real-time OLAP DBs*" like Pinot do. 

## Reading Session Summary

I need to study my data structures and algorithms because I would be lying if I completely understood everything in this chapter so far. I think at a high level I get the differences between LSM-trees and B-tree but nothing deeper than that.

Once the chapter starting talking about OLAP and Data Warehousing which is my 🍞 and 🧈. 

___

# Reading Session 08
- **date**: Tue 10/17/2023
- **time**: 5:50AM-6:00AM (10 min)
- **chapter**: Ch 03 Storage and Retrieval & Ch 04 Encoding and Evolution
- **pages**: 169-184 (15 pages)

## Summary
Finished up chapter 03 and started ch 04. I am looking towards learning about REST and RPC communication protocols as I frequently come across these terms but don't have much knowledge about them.

___

# Reading Session 09
- **date**: Wed 10/18/2023
- **time**: 8:00AM-8:05AM (5 min)
- **chapter**: Ch 04 Encoding and Evolution
- **pages**: 184-188 (4 pages)

## Notes

*encoding* and *serialization* are the same thing.

## Summary
I never heard of some of these language specific formats like `pickle` for python for encoding in-memory byte sequences into byte sequences. Briefly touched on other formats like XML,CSV & JSON with JSON being the most popular. I wonder if later in this chapter we will get to learn about gRPC. 

___

# Reading Session 10
- **date**: Thu 10/19/2023
- **time**: 9:00PM-9:03PM (3 min)
- **chapter**: Ch 04 Encoding and Evolution
- **pages**: 190-191 (2 pages)


## Summary
Read two pages to keep the streak alive. Learned a little about binary encoding.
___

# Reading Session 11
- **date**: Fri 10/20/2023
- **time**: 10:00AM-10:05AM (5 min)
- **chapter**: Ch 04 Encoding and Evolution
- **pages**: 191-197 (6 pages)


## Summary
Another short reading session where I learned a little about Thrift and Protocol buffers. This is an interesting topic to me because I've heard of a thrift server before when I worked on spark for a little bit. I wonder if these two are the same thing. I also know protocol buffers are used in gRPC. It will be nice to actually understand what these technologies actually are.

___

# Reading Session 12
- **date**: Sat 10/21/2023
- **time**: 4:20PM-4:40PM (20 min)
- **chapter**: Ch 04 Encoding and Evolution
- **pages**: 197-209 (12 pages)

## Notes
Protocol Buffers do not have a `list` or `array` datatype, but instead has a `repeated` marker for fields. Thrift has a dedicated `list` datatype but it does not allow the same evolution from single-valued to multi-valued as Protocol Buffers does but it has the advantage of supporting nested lists.

::: {.callout-important appearance="default" collapse=false title="TODO"}
Follow up to check if Avro is used for schema definition in Apache Kafka topics.
:::

JSON, CSV, XML are *textual* formats while Protocol Buffers, Thrift and Avro are *binary* formats.

Interesting to learn that Thrift and Protocol Buffers have dynamic code generation which is helpful for statically typed languages. The author points out the dynamically typed languages there is no benefit in code generation since there is no compile-time type checker. I wonder now that python has type hints if there could be an advantage. I also think it could be helpful in generating schemas for libraries [Pydantic](https://docs.pydantic.dev/latest/) which help with run time data validation. 

## Summary

This part of the book reminds of this project called [Recap](https://recap.build/) which provides the ability to transpile schema definitions in various formats. Before reading this chapter I wasn't aware of all the different schema formats so I am now seeing how someone could benefit from this tool.

___

# Reading Session 13
- **date**: Sun 10/22/2023
- **time**: 1:32PM-1:50PM (18 min)
- **chapter**: Ch 04 Encoding and Evolution
- **pages**: 209-234 (25 pages)

## Notes

> ***REST*** is not a protocol, but rather a design philosophy that builds upon the principles of HTTP... An API designed according to the principles of REST is called *RESTful*.

> ***SOAP*** is an XML-based protocol for making network API requests...aims to be independent from HTTP and avoids using most HTTP features.

The main methods discussed were:

- **Databases**: writing encodes the data and reading decodes it
- **RPC and REST APIs**: client encodes a request, server decodes it and encodes a response and client decodes the response.

  ::: {.callout-important appearance="default" collapse=false title="TODO"}
  When is it best to use gRPC vs REST APIs? My understanding was gRPC is used when you want to invoke a function from another service as if it were like a function in your own service. I recall though a customer of ours was requesting for data to delivered via gRPC opposed to a RESTful API. To me that wouldn't make sense.
  :::
  
- **Asynchronous Messaging Passing**: Nodes communicate by sending each other messages that are encoded by the send and decoded by the recipient.

## Summary
Finished chapter 04 covering many different ways of how data flows through services. So far this book feels like lego blocks with each chapter putting together the foundational pieces common distributed systems are built upon. I really enjoy this approach because it's too common nowadays to except the norm for what it is without truly understanding the many layers of abstraction we are building upon.