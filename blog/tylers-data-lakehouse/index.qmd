---
title: "Building My Own Personal Data Cabin - Part 1: The Blueprint "
description: My data cabin is a personal data lakehouse for me to go explore new technologies and ideas.  
author: "Tyler Hillery"
date: "2023-05-25"
categories: [Data Lakehouse, Iceberg]
image: "../../assets/images/data-cabin.png"
filters:
  - social-share
share:
  permalink: "https://tylerhillery.com/blog/tylers-data-lakehouse/"
  description:
  twitter: true
  facebook: false
  reddit: true
  stumble: false
  tumblr: false
  linkedin: true
  email: true
  mastodon: true
---

# **Overview**

Growing up in Wisconsin a common nomenclature is to say you are "going up north" for the weekend. This may seem odd to most given that Wisconsin is already bordering Canada so how much further north would you want to go? Now this depends where you grew up but me personally I considered "up north" a city called Tomahawk or anything further north of it. 

A common thing to do up north is to go to a cabin on a lake. It's actually a dream of most to own their own cabin on a lake somewhere up north. I too have always wanted to have my own cabin but not a *real* cabin, a data cabin. A place I can go to in my spare time to toy around with different data technologies and explore ideas I have.

You may be wondering what the difference between a data lakehouse and "data cabin" is and there isn't any difference. I am simply using the term interchangeably but I found the term quite fitting given the fact that I view real lakehouses as place you would live in everyday and have a little bit higher expectations in the build quality of the house while a cabin has more character, a little rough around the edges, a place you visit ever so often to get away from the typical mundane day to day activities. 

You can translate these comparisons to the data world, where a data lakehouse is a real production system that has been battle tested and can easily scale to support BIG data. My cabin on the other hand.. it's a place for me to get away from my daily job responsibilities to explore technologies and ideas that I find interesting. Before we get into the design of my cabin lets discuss the my definition of the data lakehouse architecture. 

## **The Data Lakehouse**
The term *Data Lakehouse* came from combining the words *Data Lake* and *Data Warehouse*. Lets first describe how I view these two terms.

### **Data Lake**
The Data Lake is the term used describe storing all your data into object storage like S3. This can be anything from structured data (parquet, CSVs etc.) to semi-structured data (JSON, XML) to unstructured data (images, audio etc,). To me that is it, that's a data lake. 

### **Data Warehouse**
The Data Warehouse is a database that is suppose to act as the coveted *single source of truth* of all data within a company. While you can use trusty old Postgres as Craig Kerstiens demonstrates here:

{{< tweet craigkerstiens 1651336279806853120 >}}

The more common players you are going to see fulfill the data warehouse role is Snowflake, BigQuery & Redshift. That's because the workloads you run on data warehouses tend to be different in nature than the workloads you run on your application database. The term for this type of workload is commonly referred to as OLAP (Online Analytical Processing) which typically involves running aggregate queries across the entire dataset where as OLTP (Online Transactional Processing) is usually more concerned with single point queries where you are reading or updating one single row. This is drastic simplification but it gets the point across.

The reason the above mention names tend to be suited for OLAP is largely because they are distributed systems designed so that they can horizontally scale resources. The storage and compute are separated as well so they scale independently of each other so you don't necessarily need to increase storage if you just want more compute resources. The distributed nature of the compute resources allows for massively parallel processing (MPP) so the database can split the work across several different nodes within the cluster. 

::: {style="font-size: 50%;text-align:center"}
##### **Snowflake Architecture** 
![](../../assets/images/snowflake-arch.png)
[*https://docs.snowflake.com/en/user-guide/intro-key-concepts*](https://docs.snowflake.com/en/user-guide/intro-key-concepts)
:::

With this all being said I should mention a database called DuckDB. DuckDB is an in-process SQL OLAP database management system meaning and it runs completely embedded within the host process without the need of separate server software. Data can be stored in persistent with a single-file database. Often referred to as the "SQLite for analytics". 

I wanted to bring up DuckDB because it completely goes against the trend from typically databases warehouses that require a distributing computing architecture. DuckDB works best around data that is less than ~200GB and has some pretty impressive benchmark results that you can look at [here](https://duckdblabs.github.io/db-benchmark/) and [here](https://www.fivetran.com/blog/how-fast-is-duckdb-really). If you want to know why it's so fast I highly recommend checking out this presentation given by Mark Rassveldt CTO of DuckDB Labs.

{{< video https://www.youtube.com/embed/bZOvAKGkzpQ aspect-ratio="4x3" >}}

The other design choice that makes these databases suited for OLAP workloads is the columnar data store. This allows for compression because typically one column is going to be of the same data type. Another benefit of the columnar data store is usually you can scan less data because a lot of times your query is only going to select a few columns from the table so it doesn't need to read those other columns. 

![](../../assets/images/row-colum-store.png)

### **Data Table Format**
Now you might be wondering how do we combine a Data Lake and a Data Warehouse? Well first lets talk about the downsides of a data lake and data warehouse so see the problem the lakehouse is trying to solve. The main disadvantage you have with a data lake is you don't have ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.

- **Atomicity** - means that all transactions either succeed or fail completely.
- **Consistency** - guarantees relate to how a given state of the data is observed by simultaneous operations.
- **Isolation** - refers to how simultaneous operations potentially conflict with one another.
- **Durability** - means that committed changes are permanent.

ACID guarantees provide data reliability and integrity. Now this is something data warehouses provide. The downside of data warehouses is that your data is stored in a managed proprietary storage format that is only queryable by the engines provided by the data warehouse provider. 

So what if we could make it so that your data lake could have ACID guarantees and a common protocol that other data processing engines could implement to interact with these tables? Enter the table format. The most common ones are [Apache Iceberg](https://iceberg.apache.org/), [Delta](https://delta.io/), [Apache Hudi](https://hudi.apache.org/). I am going to focus on Iceberg as I believe that this will be the winner of all the table formats. 

One thing I didn't quite understand at first was what is a table format? How is this different than parquet? How does this help provide ACID guarantees? These are some common questions to have when you first learn about table formats.

I highly recommend reading [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/) to understand what a table format is and the details of Iceberg. If you don't have the time some takeaways is that a table format is commonly explained as *a way to organize a dataset’s files to present them as a single “table”*^[[Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/)].

To understand how Iceberg does this we can look at three components that makeup Iceberg:

1. **Catalog**
    - A store that houses the current metadata pointer for Iceberg tables
    - Must support atomic operations for updating the current metadata point (e.g. Hadoop File System (HDFS), Hive Meta Store, Nessie, REST)
2. **Metadata Layer**
    - **Metadata File**: Stores metadata about a table which includes information such as table's schema, partition info, snapshots.
    - **Manifest List**: A list of manifest files. The manifest list has information about each manifest file that makes up that snapshot, such as the location of the manifest file, what snapshot it was added as part of, and information about the partitions it belongs to 
    - **Manifest File**: Track data files as well as additional details and statistics about each file. Each manifest file keeps track of a subset of the data files for parallelism and reuse efficiency at scale. They contain a lot of useful information that is used to improve efficiency and performance while reading the data from these data files, such as details about partition membership, record count, and lower and upper bounds of columns. Iceberg is file-format agnostic, so the manifest files also specify the file format of the data file, such as Parquet, ORC, or Avro.
3. **Data**
    - The actual data stored in file formats such as Parquet, ORC or Avro. Usually this data is stored in object storage such as S3.

::: {style="font-size: 50%;text-align:center"}
##### **Architectural diagram of the structure of an Iceberg table** 
![](../../assets/images/iceberg-metadata.png){fig-align="left" width=85%}

[*https://www.dremio.com/wp-content/uploads/2023/03/iceberg-metadata.png*](https://www.dremio.com/wp-content/uploads/2023/03/iceberg-metadata.png)
:::

The benefits of Iceberg are the following^[[The Apache Iceberg Open Table Format](https://www.dremio.com/open-source/apache-iceberg/)]:

- Transactional consistency between multiple applications where files can be added, removed or modified atomically, with full read isolation and multiple concurrent writes
- Full schema evolution to track changes to a table over time
- Time travel to query historical data and verify changes between updates
- Partition layout and evolution enabling updates to partition schemes as queries and data volumes change without relying on hidden partitions or physical directories
- Rollback to prior versions to quickly correct issues and return tables to a known good state
- Advanced planning and filtering capabilities for high performance on large data volumes

## **My Data Cabin**
Now that we have a better understanding of what a Data Lakehouse is and the benefits it provides, lets get into designing my data cabin! 

![](../../assets/images/data-cabin.png)

Now I know there is a lot going on with this diagram. That's simply because there are a lot of technologies and ideas I would like to try out! Lets start out with some of my main goals with this project.

### **The Foundation**
The core part my data cabin is going to be two main components:

#### **Data Storage Layer**
I plan on using [Tabluar](https://tabular.io/) as a managed solution for Iceberg. 

#### **Orchestrator**
The orchestrator to me is the most important component in any data stack. It is one of the only components that spans across the entire stack and helps stich everything together into one cohesive system. [Airflow](https://airflow.apache.org/) is by far the most popular orchestrator and I have always wanted to try it. I personally believe [Dagster](https://dagster.io/) will be the category winner in this space. So my plan is to setup Airflow and then imitate a migration to Dagster.

### **Two Engine dbt Project**
I would like to try to implement a [dbt](https://www.getdbt.com/) project that allows to the DEV environment to use DuckDB as the processing engine. Then when merging the changes from dev to prod enviornment is uses a different processing engine. In my example I am going to use Spark as the other query engine simply because it has the best support for reading and writing to Iceberg tables. To be able to do this I am thinking I may need to use [SQLGlot](https://sqlglot.com/sqlglot.html) to translate SQL dialects from DuckDB to SparkSQL. I also want to implement proper CI/CD pipepline that uses [data-diff](https://www.datafold.com/data-diff) and [SQLFluff](https://sqlfluff.com/). I haven't decided yet what compute enviornment I will use to manage Spark. I am thinking either AWS EMR or AWS Glue. 

### **Custom Open Source Connectors**
My goal here is that I want to build two brand new open source connectors. One using [Airbyte](https://airbyte.com/) and another using [Meltano](https://meltano.com/). I'd like to ingest then setup two batch pipelines from new connectors and load it into Tabluar. If Tabluar or Iceberg is not currently support than I will look at building those source connectors. 

### **Real Time Data Pipelines**

#### **Real Time Data Pipeline #1**
I want to build a real time data pipeline that ingests data from the [Coinbase websocket](https://docs.cloud.coinbase.com/prime/docs/websocket-feed), uses [Buz](https://buz.dev) to validate the schema of the data from Coinbase before it sends it to [Redpanda](https://redpanda.com/) a streaming data platform. After that I will do some data stream processing with [Bytewax](https://bytewax.io/) reading from one Redpanda topic and writing to another one. Then I will load the data into [Tabluar](https://tabular.io/) which is a manage Iceberg solution. Lastly, I want to try to create a [Coinbase Pro](https://pro.coinbase.com/) clone as shown in the image below. I am undecided how I want to implement the frontend but I am thinking about using a JavaScript framework and data viz library like [Charts.js](https://www.chartjs.org/). 

![](../../assets/images/coinbase-pro.png)

#### **Real Time Data Pipeline #2**
In the second real time data pipeline I want to set up [snowplow](https://snowplow.io/) to create, track and collect events such as impressions, clicks, video playback (or even something other custom). Snowplow with send the data to Buz to validate the schema of the data before sending it to [Kafka](https://kafka.apache.org/). After it lands in a Kafka topic I will use a streaming database called [Materialize](https://materialize.com/) to do some stream processing before writing it back to another Kafka topic and loading it into Tabluar. 

#### **Real Time Data Pipeline #3**
The last real time data pipeline implement Change Data Capture (CDC) using [Debezium](https://debezium.io/) to ingest changes that occur to a Postgres database that will be hosted by [Crunchy Data](https://www.crunchydata.com/). These events then will get sent to a Kafka topic and then [Flink](https://flink.apache.org/) will ingest that topic and write the data into Tabular. 

### **Semantic Layer**
I think semantic layers will start to become more popular and I want to try out one of the most popular ones [cube](https://cube.dev/).

### **BI Layer**
These are several BI tools that interest me and I would like to build at least one dashboard in each. 

- [Rill](https://www.rilldata.com/)
- [Evidence](https://evidence.dev/)
- [Metabase](https://www.metabase.com/) 
- [Streamlit](https://streamlit.io/)

## Conclusion 
Now that we have a blueprint created it's time to start building the data cabin! In my next post I will be writing about my experience on setting up Tabluar with some sample data and running our first queries.